{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# Public estimators\n",
    "# =============================================================================\n",
    "\n",
    "def AdaBoost_R2_T(trans_S, response_S, test, weight,frozen_N, N = 20):\n",
    "\n",
    "    trans_data =  copy.deepcopy(trans_S)\n",
    "    trans_response =  copy.deepcopy(response_S)\n",
    "\n",
    "    row_S = trans_S.shape[0]\n",
    "    row_T = test.shape[0]\n",
    "\n",
    "    test_data = np.concatenate((trans_data, test), axis=0)\n",
    "    weights = copy.deepcopy(weight)\n",
    "    # initilize data weights\n",
    "    _weights = weights / sum(weights)\n",
    "\n",
    "    # Save prediction responses and bata_t\n",
    "    bata_T = np.zeros(N)\n",
    "    result_response = np.ones([row_S + row_T, N])\n",
    "\n",
    "    # Save the prediction responses of test data \n",
    "    predict = np.zeros(row_T)\n",
    "\n",
    "    trans_data = np.asarray(trans_data, order='C')\n",
    "    trans_response = np.asarray(trans_response, order='C')\n",
    "    test_data = np.asarray(test_data, order='C')\n",
    "\n",
    "    Total_S_weight = np.sum(weights[-frozen_N:])\n",
    "    for i in range(N):\n",
    "        _weights = calculate_P(_weights, frozen_N,Total_S_weight)\n",
    "        result_response[:, i] = train_reg(trans_data, trans_response, test_data, _weights)[0]\n",
    "        error_rate = calculate_error_rate(response_S, result_response[0: row_S, i],_weights)\n",
    "        if error_rate > 0.5 or error_rate <= 1e-10: break\n",
    "\n",
    "        bata_T[i] = error_rate / (1 - error_rate)\n",
    "\n",
    "        # Changing the data weights of unfrozen training data\n",
    "        D_t = np.abs(result_response[frozen_N:row_S, i] - response_S[frozen_N:row_S]).max()\n",
    "        for j in range(row_S - frozen_N):\n",
    "            _weights[frozen_N + j] = _weights[frozen_N + j] * np.power(bata_T[i], (1-np.abs(result_response[frozen_N + j, i] - response_S[frozen_N+j])/D_t))\n",
    "    \n",
    "    \n",
    "    Cal_res = result_response[row_S:,:]\n",
    "    # Sort the predictions\n",
    "    sorted_idx = np.argsort(Cal_res, axis=1)\n",
    "\n",
    "    # Find index of median prediction for each sample\n",
    "    weight_cdf = np.cumsum(bata_T[sorted_idx], axis=1)\n",
    "    # return True - False\n",
    "    median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n",
    "    median_idx = median_or_above.argmax(axis=1)\n",
    "\n",
    "    median_estimators = sorted_idx[np.arange(row_T), median_idx]\n",
    "    for j in range(row_T):\n",
    "        predict[j] = Cal_res[j,median_estimators[j]]\n",
    "    return predict\n",
    "\n",
    "def calculate_P(weights,frozen_N,Total_S_weight):\n",
    "    total = np.sum(weights[-frozen_N:])\n",
    "    weights[-frozen_N:] / total * Total_S_weight\n",
    "    return np.asarray(weights, order='C')\n",
    "\n",
    "def train_reg(trans_data, trans_response, test_data, weights):\n",
    "    # In order to ensure that the results are not random,\n",
    "    # the weights are adjusted by the built-in method \n",
    "    # reg = DecisionTreeRegressor(max_depth=2,splitter='random',max_features=\"log2\",random_state=0)\n",
    "    # reg.fit(trans_data, trans_response,sample_weight = weights)\n",
    "    # return reg.predict(test_data)\n",
    "    print(trans_response.shape)\n",
    "    return base_LSTM(trans_data, trans_response, weights).predict(test_data,)\n",
    "def base_LSTM(train, response, weight, N=1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(32, input_shape=(train.shape[1], 1)))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    train = np.reshape(train, (train.shape[0], train.shape[1], 1))\n",
    "    model.fit(train, response, epochs=N, batch_size=1, verbose=2)\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_error_rate(response_R, response_H, weight):\n",
    "    total = np.abs(response_R - response_H).max()\n",
    "    return np.sum(weight[:] * np.abs(response_R - response_H) / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# Public estimators\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def Two_stage_TrAdaboost_R2(trans_S, Multi_trans_A, response_S, Multi_response_A, test, steps_S, N):\n",
    "    \"\"\"Boosting for Regression Transfer\n",
    "\n",
    "    Please feel free to open issues in the Github : https://github.com/Bin-Cao/TrAdaboost\n",
    "    or \n",
    "    contact Bin Cao (bcao@shu.edu.cn)\n",
    "    in case of any problems/comments/suggestions in using the code. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trans_S : feature matrix of same-distribution training data\n",
    "\n",
    "    Multi_trans_A : dict, feature matrix of diff-distribution training data\n",
    "    e.g.,\n",
    "    Multi_trans_A = {\n",
    "    'trans_A_1' :  data_1 , \n",
    "    'trans_A_2' : data_2 ,\n",
    "    ......\n",
    "    }\n",
    "\n",
    "    response_S : responses of same-distribution training data, real number\n",
    "\n",
    "    Multi_response_A : dict, responses of diff-distribution training data, real number\n",
    "    e.g.,\n",
    "    Multi_response_A = {\n",
    "    'response_A_1' :  response_1 , \n",
    "    'response_A_2' : response_2 ,\n",
    "    ......\n",
    "    }\n",
    "\n",
    "    test : feature matrix of test data\n",
    "\n",
    "    steps_S: int, the number of steps (see Algorithm 3)\n",
    "\n",
    "    N: int, the number of estimators in AdaBoost_R2_T\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    # same-distribution training data\n",
    "    tarin_data = pd.read_csv('M_Sdata.csv')\n",
    "    # two diff-distribution training data\n",
    "    A1_tarin_data = pd.read_csv('M_Adata1.csv')\n",
    "    A2_tarin_data = pd.read_csv('M_Adata2.csv')\n",
    "    # test data\n",
    "    test_data = pd.read_csv('M_Tdata.csv')\n",
    "\n",
    "    Multi_trans_A = {\n",
    "    'trans_A_1' : A1_tarin_data.iloc[:,:-1],\n",
    "    'trans_A_2' : A2_tarin_data.iloc[:,:-1]\n",
    "    }\n",
    "    Multi_response_A = {\n",
    "    'response_A_1' :  A1_tarin_data.iloc[:,-1] , \n",
    "    'response_A_2' :  A2_tarin_data.iloc[:,-1] ,\n",
    "    }\n",
    "    trans_S = tarin_data.iloc[:,:-1]\n",
    "    response_S = tarin_data.iloc[:, -1]\n",
    "    test = test_data.iloc[:,:-1]\n",
    "    N = 20\n",
    "    steps_S = 10\n",
    "    Two_stage_TrAdaboost_R2(trans_S, Multi_trans_A, response_S, Multi_response_A, test, steps_S, N)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Algorithm 3 \n",
    "    Pardoe, D., & Stone, P. (2010, June). \n",
    "    Boosting for regression transfer. \n",
    "    In Proceedings of the 27th International Conference \n",
    "    on International Conference on Machine Learning (pp. 863-870).\n",
    "\n",
    "    \"\"\"\n",
    "    # prepare trans_A\n",
    "    trans_A = list(Multi_trans_A.values())[0]\n",
    "    if len(Multi_trans_A) == 1:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(len(Multi_trans_A)-1):\n",
    "            p = i + 1\n",
    "            trans_A = np.concatenate((trans_A, list(Multi_trans_A.values())[p]), axis=0)\n",
    "    # prepare response_A\n",
    "    response_A = list(Multi_response_A.values())[0]\n",
    "    if len(Multi_response_A) == 1:\n",
    "        pass \n",
    "    else:\n",
    "        for i in range(len(Multi_response_A)-1):\n",
    "            p = i + 1\n",
    "            response_A = np.concatenate((response_A, list(Multi_response_A.values())[p]), axis=0)\n",
    "   \n",
    "    trans_data = np.concatenate((trans_A, trans_S), axis=0)\n",
    "    trans_response = np.concatenate((response_A, response_S), axis=0)\n",
    "\n",
    "    row_A = trans_A.shape[0]\n",
    "    row_S = trans_S.shape[0]\n",
    "\n",
    "    # Initialize the weights\n",
    "    weight  = np.ones(row_A+row_S)/(row_A+row_S)\n",
    "    bata_T = np.zeros(steps_S)\n",
    "    \n",
    "    print ('params initial finished.')\n",
    "    print('='*60)\n",
    "\n",
    "    # generate a pool of AdaBoost_R2_T\n",
    "    AdaBoost_pre = []\n",
    "    model_error = []\n",
    "    for i in range(steps_S):\n",
    "        res_ = AdaBoost_R2_T(trans_data, trans_response, test, weight,row_A, N )\n",
    "        AdaBoost_pre.append(res_)\n",
    "        LOOCV_MSE = LOOCV_test(trans_data, trans_response,  weight,row_A, N)\n",
    "        model_error.append(LOOCV_MSE)\n",
    "        \"\"\"\n",
    "        The paper says that:\n",
    "        In addition, it is not necessary to progress through all S steps once it has been determined that errors are increasing.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(model_error) > 2 and model_error[-1] > model_error[-2]:\n",
    "            steps_S = i\n",
    "            break\n",
    "\n",
    "        \"\"\"\n",
    "        # update the data weights\n",
    "        # weight resampling \n",
    "        cdf = np.cumsum(weight)\n",
    "        cdf_ = cdf / cdf[-1]\n",
    "        uniform_samples = np.random.random_sample(len(trans_data))\n",
    "        bootstrap_idx = cdf_.searchsorted(uniform_samples, side='right')\n",
    "        # searchsorted returns a scalar\n",
    "        bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
    "        reg = DecisionTreeRegressor(max_depth=2,splitter='random',max_features=\"log2\",random_state=0)\n",
    "        reg.fit(trans_data[bootstrap_idx], trans_response[bootstrap_idx])\n",
    "        pre_res = reg.predict(trans_data)\n",
    "        E_t = calculate_error_rate(trans_response, pre_res, weight)\n",
    "        \"\"\"\n",
    "        # In order to ensure that the results are not random,\n",
    "        # the weights are adjusted by the built-in method \n",
    "        reg = DecisionTreeRegressor(max_depth=2,splitter='random',max_features=\"log2\",random_state=0)\n",
    "        reg.fit(trans_data, trans_response,sample_weight = weight)\n",
    "        pre_res = reg.predict(trans_data)\n",
    "        E_t = calculate_error_rate(trans_response, pre_res, weight)\n",
    "\n",
    "        bata_T[i] =  E_t / (1 - E_t)\n",
    "\n",
    "        # Changing the data weights of same-distribution training data\n",
    "        total_w_S = row_S/(row_A+row_S) + i/(steps_S-1)*(1 - row_S/(row_A+row_S))\n",
    "        weight[row_A : row_A+row_S] =  (weight[row_A : row_A+row_S] / weight[row_A : row_A+row_S].sum()) * total_w_S\n",
    "        # Changing the data weights of diff-distribution training data\n",
    "        \"\"\"\n",
    "        # for saving computation power, we apply the strategy in MultiSourceTrAdaBoost to update the weights\n",
    "        # see: 10.1109/CVPR.2010.5539857\n",
    "        for j in range(row_A):\n",
    "            weight[j] = weight[j] * np.exp(-bata_T[i] * np.abs(trans_response[j] - pre_res[j]))\n",
    "        weight[0:row_A] =  weight[0:row_A] * (1-total_w_S) / weight[0:row_A].sum()\n",
    "        \"\"\"  \n",
    "        beta_t = binary_search(total_w_S,weight,trans_response,pre_res,row_A,beta_t_range = (0.01,1,0.01),tal=0.03)\n",
    "        if beta_t == None:\n",
    "            for j in range(row_A):\n",
    "                weight[j] = weight[j] * np.exp(-bata_T[i] * np.abs(trans_response[j] - pre_res[j]))\n",
    "            weight[0:row_A] =  weight[0:row_A] * (1-total_w_S) / weight[0:row_A].sum()\n",
    "        else:\n",
    "            D_t = np.abs(trans_response[0:row_A] - pre_res[0:row_A]).max()\n",
    "            for j in range(row_A):\n",
    "                weight[j] = weight[j] * np.power(beta_t, np.abs(trans_response[j] - pre_res[j])/D_t)\n",
    "            weight[0:row_A] =  weight[0:row_A] * (1-total_w_S) / weight[0:row_A].sum()\n",
    "\n",
    "        print('Iter {}-th result :'.format(i))\n",
    "        print('{} AdaBoost_R2_T model has been instantiated :'.format(len(model_error)), '|| E_t :', E_t )\n",
    "        print('The LOOCV MSE on TARGET DOMAIN DATA : ',LOOCV_MSE)\n",
    "        print('The beta_t calculated by binary search is : ',beta_t)\n",
    "        print('-'*60)\n",
    "      \n",
    "    model_error = np.array(model_error)\n",
    "    min_index = np.random.choice(np.flatnonzero(model_error == model_error.min()))\n",
    "    print('Two_stage_TrAdaboost_R2 is done')\n",
    "    print('='*60)\n",
    "    print('The minimum mean square error :',model_error[min_index])\n",
    "    print('The prediction responses of test data are :')\n",
    "    print(AdaBoost_pre[min_index])\n",
    "    return AdaBoost_pre[min_index]\n",
    "\n",
    "\n",
    "def LOOCV_test(trans_data, trans_response, weight,row_A, N):\n",
    "    loo = LeaveOneOut()\n",
    "    X = np.array(trans_data)\n",
    "    Y = np.array(trans_response)\n",
    "    y_pre_loocv = []\n",
    "    cal = 0\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, _ = Y[train_index], Y[test_index]\n",
    "        w_train, _ = weight[train_index], weight[test_index]\n",
    "        if cal <= row_A-1:\n",
    "            y_pre = AdaBoost_R2_T(X_train, y_train, X_test, w_train,row_A-1, N )\n",
    "        else:\n",
    "            y_pre = AdaBoost_R2_T(X_train, y_train, X_test, w_train,row_A, N )\n",
    "        y_pre_loocv.append(y_pre[0])\n",
    "    return mean_squared_error(trans_response[row_A:],y_pre_loocv[row_A:])\n",
    "\n",
    "\n",
    "def calculate_error_rate(response_R, response_H, weight):\n",
    "    total = np.abs(response_R - response_H).max()\n",
    "    return np.sum(weight[:] * np.abs(response_R - response_H) / total)\n",
    "\n",
    "# binary_search strategy\n",
    "def binary_search(total_w_S,__weight,trans_response,pre_res,row_A,beta_t_range = (0.01,1,0.01),tal=0.03):\n",
    "    # beta_t_range is the search range of beta_t, default = (0.01,1,0.01)\n",
    "    # viz., beta_t is searched in the interval of 0 to 1, with the step of 0.01 by binary_search\n",
    "    \n",
    "    D_t = np.abs(trans_response[0:row_A] - pre_res[0:row_A]).max()\n",
    "    _list = np.arange(beta_t_range[0],beta_t_range[1],beta_t_range[2])\n",
    "    low = 0          \n",
    "    high = len(_list)-1\n",
    "    while low <= high:   \n",
    "        weight = copy.deepcopy(__weight) \n",
    "        mid = int(np.floor((low+high)/2))\n",
    "        guess = _list[mid]\n",
    "        # test beta_t\n",
    "        for j in range(row_A):\n",
    "            weight[j] = weight[j] * np.power(guess, np.abs(trans_response[j] - pre_res[j])/D_t)\n",
    "        diff = (1-total_w_S) -  weight[0:row_A].sum()\n",
    "        if abs(diff) <= tal:     \n",
    "            return guess\n",
    "        # exceed the convergence crtiterion\n",
    "        elif diff > 0:\n",
    "            low = mid + 1   \n",
    "        else:  \n",
    "            high = mid -1               \n",
    "      \n",
    "    print(\"UNABLE TO COVERGEE IN BINARY SEARCHING\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Domain_0 = pd.read_csv('C:/Users/acer/OneDrive - Nuce/Desktop/HỌC/ML/Transfer Learning/Paper/Data/1_1/Domain0.csv')\n",
    "Domain_1 = pd.read_csv('C:/Users/acer/OneDrive - Nuce/Desktop/HỌC/ML/Transfer Learning/Paper/Data/1_1/Domain1.csv')\n",
    "Domain_2 = pd.read_csv('C:/Users/acer/OneDrive - Nuce/Desktop/HỌC/ML/Transfer Learning/Paper/Data/1_1/Domain2.csv')\n",
    "Domain_3 = pd.read_csv('C:/Users/acer/OneDrive - Nuce/Desktop/HỌC/ML/Transfer Learning/Paper/Data/1_1/Domain3.csv') \n",
    "Domain_Target = pd.read_csv('C:/Users/acer/OneDrive - Nuce/Desktop/HỌC/ML/Transfer Learning/Paper/Data/1_1/Domain4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_label = 1\n",
    "n_nums_label = -1*nums_label\n",
    "Multi_trans_A = {\n",
    "    'trans_A_1':Domain_0.iloc[:,:n_nums_label],\n",
    "    'trans_A_2':Domain_1.iloc[:,:n_nums_label],\n",
    "    'trans_A_3':Domain_2.iloc[:,:n_nums_label],\n",
    "    'trans_A_4':Domain_3.iloc[:,:n_nums_label]\n",
    "}\n",
    "Multi_response_A = {\n",
    "    'response_A_1':Domain_0.iloc[:,n_nums_label:],\n",
    "    'response_A_2':Domain_1.iloc[:,n_nums_label:],\n",
    "    'response_A_3':Domain_2.iloc[:,n_nums_label:],\n",
    "    'response_A_4':Domain_3.iloc[:,n_nums_label:]\n",
    "}\n",
    "trans_S = Domain_Target[:4].iloc[:,:n_nums_label]\n",
    "response_S = Domain_Target[:4]. iloc[:,n_nums_label:]\n",
    "test = Domain_Target[4:].iloc[:,:n_nums_label]\n",
    "steps_S = 10\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.665653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.697146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1\n",
       "0  0.665653\n",
       "1  0.727490\n",
       "2  0.000000\n",
       "3  0.697146"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.522149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.187979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.299171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.522149\n",
       "1  0.187979\n",
       "2  0.372427\n",
       "3  0.299171"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(response_S, trans_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params initial finished.\n",
      "============================================================\n",
      "(44, 1)\n",
      "44/44 - 3s - loss: 0.1962 - 3s/epoch - 59ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1971 - 2s/epoch - 50ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1697 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1982 - 2s/epoch - 36ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020C52309AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2140 - 2s/epoch - 35ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020C4EC2DF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1964 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1952 - 2s/epoch - 58ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2187 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2027 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2351 - 2s/epoch - 49ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2102 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1889 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2191 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2044 - 3s/epoch - 67ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2110 - 2s/epoch - 52ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2035 - 3s/epoch - 61ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1985 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2079 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2119 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1745 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2095 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2253 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1958 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2099 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1824 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2218 - 3s/epoch - 75ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1678 - 2s/epoch - 50ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1844 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1909 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1902 - 2s/epoch - 56ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1865 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2058 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2181 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2085 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2138 - 2s/epoch - 54ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2195 - 2s/epoch - 52ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2127 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1867 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1881 - 2s/epoch - 50ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2110 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2179 - 3s/epoch - 77ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1970 - 2s/epoch - 46ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1759 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2034 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2201 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "Iter 0-th result :\n",
      "1 AdaBoost_R2_T model has been instantiated : || E_t : 11.673892362115422\n",
      "The LOOCV MSE on TARGET DOMAIN DATA :  0.2149659283907462\n",
      "The beta_t calculated by binary search is :  0.87\n",
      "------------------------------------------------------------\n",
      "(44, 1)\n",
      "44/44 - 3s - loss: 0.1825 - 3s/epoch - 67ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1996 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1675 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2143 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2056 - 1s/epoch - 32ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2023 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1878 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1724 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1915 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1854 - 1s/epoch - 32ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 4s - loss: 0.2056 - 4s/epoch - 98ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1939 - 3s/epoch - 59ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2074 - 3s/epoch - 60ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2009 - 3s/epoch - 73ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1980 - 3s/epoch - 66ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1835 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1876 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2033 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2334 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1919 - 2s/epoch - 45ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1824 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2326 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2006 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2209 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1946 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1843 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2118 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2237 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1970 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1784 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1864 - 2s/epoch - 45ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2019 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1931 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1935 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2187 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1990 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1969 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1803 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1777 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1721 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2229 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2023 - 2s/epoch - 53ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2094 - 3s/epoch - 71ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2196 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2165 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "Iter 1-th result :\n",
      "2 AdaBoost_R2_T model has been instantiated : || E_t : 11.692475635483476\n",
      "The LOOCV MSE on TARGET DOMAIN DATA :  0.23805022060433992\n",
      "The beta_t calculated by binary search is :  0.5\n",
      "------------------------------------------------------------\n",
      "(44, 1)\n",
      "44/44 - 2s - loss: 0.2045 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2159 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1629 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2089 - 2s/epoch - 53ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2236 - 3s/epoch - 68ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1817 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1929 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2178 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2178 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2091 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1990 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2027 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1905 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2064 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1909 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1766 - 2s/epoch - 58ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2218 - 2s/epoch - 56ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2140 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2141 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1858 - 3s/epoch - 80ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1939 - 2s/epoch - 56ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2021 - 3s/epoch - 66ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2031 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2108 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2020 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1892 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1918 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1872 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1921 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1780 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1966 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2094 - 2s/epoch - 49ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1986 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2252 - 2s/epoch - 49ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1848 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1990 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2032 - 2s/epoch - 58ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1891 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1916 - 2s/epoch - 45ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2173 - 2s/epoch - 49ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2036 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2082 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2308 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2114 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1876 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "Iter 2-th result :\n",
      "3 AdaBoost_R2_T model has been instantiated : || E_t : 12.049216970525732\n",
      "The LOOCV MSE on TARGET DOMAIN DATA :  0.23802441038069982\n",
      "The beta_t calculated by binary search is :  0.5\n",
      "------------------------------------------------------------\n",
      "(44, 1)\n",
      "44/44 - 3s - loss: 0.2136 - 3s/epoch - 63ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1991 - 3s/epoch - 62ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2109 - 2s/epoch - 52ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2242 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2056 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 1s 2ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1996 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2173 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2023 - 1s/epoch - 35ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2237 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2008 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1931 - 3s/epoch - 74ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2178 - 3s/epoch - 61ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1931 - 2s/epoch - 55ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1868 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2037 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1733 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1908 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 4s - loss: 0.2353 - 4s/epoch - 87ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1807 - 2s/epoch - 57ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1934 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2004 - 1s/epoch - 31ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2093 - 2s/epoch - 45ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1906 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1906 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1847 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2234 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1878 - 1s/epoch - 31ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1826 - 1s/epoch - 32ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2090 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2014 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2051 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1936 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2191 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1977 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1906 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2166 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2112 - 2s/epoch - 49ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1758 - 1s/epoch - 35ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2353 - 2s/epoch - 43ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1987 - 2s/epoch - 41ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2069 - 2s/epoch - 36ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2048 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1860 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1920 - 1s/epoch - 32ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1981 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Iter 3-th result :\n",
      "4 AdaBoost_R2_T model has been instantiated : || E_t : 12.388737191137054\n",
      "The LOOCV MSE on TARGET DOMAIN DATA :  0.2173263837070889\n",
      "The beta_t calculated by binary search is :  0.5\n",
      "------------------------------------------------------------\n",
      "(44, 1)\n",
      "44/44 - 1s - loss: 0.1993 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1980 - 1s/epoch - 29ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1900 - 1s/epoch - 32ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2058 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2181 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2168 - 2s/epoch - 38ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2042 - 3s/epoch - 78ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2006 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2098 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1847 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 4s - loss: 0.2278 - 4s/epoch - 93ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2178 - 3s/epoch - 61ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2180 - 3s/epoch - 81ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1888 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2184 - 2s/epoch - 47ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2253 - 2s/epoch - 51ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1864 - 1s/epoch - 31ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2064 - 2s/epoch - 48ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1645 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1947 - 3s/epoch - 60ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2030 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1873 - 2s/epoch - 42ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1831 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2013 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1964 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1952 - 2s/epoch - 40ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.1800 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1937 - 3s/epoch - 63ms/step\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1936 - 3s/epoch - 60ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1885 - 2s/epoch - 55ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2139 - 2s/epoch - 52ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2004 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 1s 2ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2072 - 2s/epoch - 39ms/step\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2227 - 1s/epoch - 34ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2127 - 2s/epoch - 37ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2170 - 3s/epoch - 77ms/step\n",
      "2/2 [==============================] - 1s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2181 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2027 - 2s/epoch - 35ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 1s - loss: 0.2148 - 1s/epoch - 33ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1835 - 2s/epoch - 44ms/step\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.2156 - 2s/epoch - 55ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2045 - 3s/epoch - 62ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.2238 - 3s/epoch - 65ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 3s - loss: 0.1787 - 3s/epoch - 60ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "(43, 1)\n",
      "43/43 - 2s - loss: 0.1884 - 2s/epoch - 46ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "Two_stage_TrAdaboost_R2 is done\n",
      "============================================================\n",
      "The minimum mean square error : 0.2149659283907462\n",
      "The prediction responses of test data are :\n",
      "[0.1935627 0.1935627 0.1935627 0.1935627 0.1935627 0.1935627]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1935627, 0.1935627, 0.1935627, 0.1935627, 0.1935627, 0.1935627])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Two_stage_TrAdaboost_R2(trans_S, Multi_trans_A, response_S,\n",
    "                        Multi_response_A, test, steps_S, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
